"""
This script creates a set of commonly used features using the functions in
common.feature_utils, which serve as a set of baseline features.
Feel free to write your own feature engineering code to create new features by
calling the feature_utils functions with alternative parameters.
"""
import os, sys, getopt
from functools import reduce
from datetime import timedelta
import datetime
import pandas as pd
import numpy as np

import localpath
from energy_load.GEFCom2017_D_Prob_MT_hourly.common.benchmark_paths\
    import DATA_DIR, SUBMISSIONS_DIR

print('Data directory used: {}'.format(DATA_DIR))

TRAIN_DATA_DIR = os.path.join(DATA_DIR, 'train')
TEST_DATA_DIR = os.path.join(DATA_DIR, 'test')

TRAIN_BASE_FILE = 'train_base.csv'
TRAIN_FILE_PREFIX = 'train_round_'
TEST_FILE_PREFIX = 'test_round_'
NUM_ROUND = 6

DATETIME_COLNAME = 'Datetime'
HOLIDAY_COLNAME = 'Holiday'

DATETIME_FORMAT = '%Y-%m-%d %H:%M:%S'


ALLOWED_TIME_COLUMN_TYPES = [pd.Timestamp, pd.DatetimeIndex,
                             datetime.datetime, datetime.date]


def is_datetime_like(x):
    return any(isinstance(x, col_type)
               for col_type in ALLOWED_TIME_COLUMN_TYPES)


def hour_of_day(datetime_col):
    return datetime_col.dt.hour


def month_of_year(date_time_col):
    return date_time_col.dt.month

def fourier_approximation(t, n, period):
    """
    Generic helper function for create Fourier Series at different
    harmonies(n) and periods.
    """
    x = n * 2 * np.pi * t/period
    x_sin = np.sin(x)
    x_cos = np.cos(x)

    return x_sin, x_cos


def annual_fourier(datetime_col, n_harmonics):
    day_of_year = datetime_col.dt.dayofyear

    output_dict = {}
    for n in range(1, n_harmonics+1):
        sin, cos = fourier_approximation(day_of_year, n, 365.24)

        output_dict['annual_sin_'+str(n)] = sin
        output_dict['annual_cos_'+str(n)] = cos

    return output_dict


def weekly_fourier(datetime_col, n_harmonics):
    day_of_week = datetime_col.dt.dayofweek

    output_dict = {}
    for n in range(1, n_harmonics+1):
        sin, cos = fourier_approximation(day_of_week, n, 7)

        output_dict['weekly_sin_'+str(n)] = sin
        output_dict['weekly_cos_'+str(n)] = cos

    return output_dict


def daily_fourier(datetime_col, n_harmonics):
    hour_of_day = datetime_col.dt.hour + 1

    output_dict = {}
    for n in range(1, n_harmonics+1):
        sin, cos = fourier_approximation(hour_of_day, n, 24)

        output_dict['daily_sin_'+str(n)] = sin
        output_dict['daily_cos_'+str(n)] = cos

    return output_dict


def same_week_day_hour_lag(datetime_col, value_col, n_years=3,
                           week_window=1, agg_func='mean',
                           output_colname='SameWeekHourLag'):
    """
    Create a lag feature by averaging values of and around the same week,
    same day of week, and same hour of day, of previous years.
    :param datetime_col: Datetime column
    :param value_col: Feature value column to create lag feature from
    :param n_years: Number of previous years data to use
    :param week_window:
        Number of weeks before and after the same week to
        use, which should help reduce noise in the data
    :param agg_func: aggregation function to apply on multiple previous values
    :param output_colname: name of the output lag feature column
    """

    if not is_datetime_like(datetime_col):
        datetime_col = pd.to_datetime(datetime_col, format=DATETIME_FORMAT)
    min_time_stamp = min(datetime_col)
    max_time_stamp = max(datetime_col)

    df = pd.DataFrame({'Datetime': datetime_col, 'value': value_col})
    df.set_index('Datetime', inplace=True)

    week_lag_base = 52
    week_lag_last_year = list(range(week_lag_base - week_window,
                              week_lag_base + week_window + 1))
    week_lag_all = []
    for y in range(n_years):
        week_lag_all += [x + y * 52 for x in week_lag_last_year]

    week_lag_cols = []
    for w in week_lag_all:
        if (max_time_stamp - timedelta(weeks=w)) >= min_time_stamp:
            col_name = 'week_lag_' + str(w)
            week_lag_cols.append(col_name)

            lag_datetime = df.index.get_level_values(0) - timedelta(weeks=w)
            valid_lag_mask = lag_datetime >= min_time_stamp

            df[col_name] = np.nan

            df.loc[valid_lag_mask, col_name] = \
                df.loc[lag_datetime[valid_lag_mask], 'value'].values

    # Additional aggregation options will be added as needed
    if agg_func == 'mean':
        df[output_colname] = round(df[week_lag_cols].mean(axis=1))

    return df[[output_colname]]


def same_day_hour_lag(datetime_col, value_col, n_years=3,
                      day_window=1, agg_func='mean',
                      output_colname='SameDayHourLag'):
    """
    Create a lag feature by averaging values of and around the same day of
    year, and same hour of day, of previous years.
    :param datetime_col: Datetime column
    :param value_col: Feature value column to create lag feature from
    :param n_years: Number of previous years data to use
    :param day_window:
        Number of days before and after the same day to
        use, which should help reduce noise in the data
    :param agg_func: aggregation function to apply on multiple previous values
    :param output_colname: name of the output lag feature column
    """

    if not is_datetime_like(datetime_col):
        datetime_col = pd.to_datetime(datetime_col, format=DATETIME_FORMAT)
    min_time_stamp = min(datetime_col)
    max_time_stamp = max(datetime_col)

    df = pd.DataFrame({'Datetime': datetime_col, 'value': value_col})
    df.set_index('Datetime', inplace=True)

    day_lag_base = 365
    day_lag_last_year = list(range(day_lag_base - day_window,
                                   day_lag_base + day_window + 1))
    day_lag_all = []
    for y in range(n_years):
        day_lag_all += [x + y * 365 for x in day_lag_last_year]

    day_lag_cols = []
    for d in day_lag_all:
        if (max_time_stamp - timedelta(days=d)) >= min_time_stamp:
            col_name = 'day_lag_' + str(d)
            day_lag_cols.append(col_name)

            lag_datetime = df.index.get_level_values(0) - timedelta(days=d)
            valid_lag_mask = lag_datetime >= min_time_stamp

            df[col_name] = np.nan

            df.loc[valid_lag_mask, col_name] = \
                df.loc[lag_datetime[valid_lag_mask], 'value'].values

    # Additional aggregation options will be added as needed
    if agg_func == 'mean':
        df[output_colname] = round(df[day_lag_cols].mean(axis=1))

    return df[[output_colname]]


def create_basic_features(input_df, datetime_colname):
    """
    This helper function uses the functions in common.feature_utils to
    create a set of basic features which are independently created for each
    row, i.e. no lag features or rolling window features.
    """

    output_df = input_df.copy()
    if not is_datetime_like(output_df[datetime_colname]):
        output_df[datetime_colname] = \
            pd.to_datetime(output_df[datetime_colname], format=DATETIME_FORMAT)
    datetime_col = output_df[datetime_colname]

    # Basic temporal features
    output_df['Hour'] = hour_of_day(datetime_col)
    output_df['MonthOfYear'] = month_of_year(datetime_col)

    # Fourier approximation features
    annual_fourier_approx = annual_fourier(datetime_col, n_harmonics=3)
    weekly_fourier_approx = weekly_fourier(datetime_col, n_harmonics=3)

    for k, v in annual_fourier_approx.items():
        output_df[k] = v

    for k, v in weekly_fourier_approx.items():
        output_df[k] = v

    return output_df


def create_advanced_features(train_df, test_df, datetime_colname,
                             holiday_colname=None):
    """
    This helper function uses the functions in common.feature_utils to
    create a set of advanced features. These features could depend on other
    rows in two ways:
    1) Lag or rolling window features depend on values of previous time points.
    2) Normalized features depend on the value range of the entire feature
    column.
    Therefore, the train_df and test_df are concatenated to create these
    features.
    NOTE: test_df can not contain any values that are unknown at
    forecasting creation time to avoid data leakage from the future. For
    example, it can contain the timestamps, zone, holiday, forecasted
    temperature, but it MUST NOT contain things like actual temperature,
    actual load, etc.
    """
    output_df = pd.concat([train_df, test_df], sort=True)
    if not is_datetime_like(output_df[datetime_colname]):
        output_df[datetime_colname] = \
            pd.to_datetime(output_df[datetime_colname], format=DATETIME_FORMAT)

    # Load lag feature based on previous years' load
    same_week_day_hour_load_lag = \
        output_df[[datetime_colname, 'DEMAND', 'Zone']].groupby('Zone').apply(
            lambda g: same_week_day_hour_lag(g[datetime_colname],
                                             g['DEMAND'],
                                             output_colname='LoadLag'))
    same_week_day_hour_load_lag.reset_index(inplace=True)

    same_day_hour_drybulb_lag = \
        output_df[[datetime_colname, 'DryBulb', 'Zone']].groupby('Zone').apply(
            lambda g: same_day_hour_lag(g[datetime_colname], g['DryBulb'],
                                        output_colname='DryBulbLag'))
    same_day_hour_drybulb_lag.reset_index(inplace=True)

    # Put everything together
    output_df = reduce(
        lambda left, right: pd.merge(left, right, on=[datetime_colname, 'Zone']),
        [output_df, same_week_day_hour_load_lag, same_day_hour_drybulb_lag])

    # Split train and test data and return separately
    train_end = max(train_df[datetime_colname])
    output_df_train = output_df.loc[output_df[datetime_colname] <= train_end, ]
    output_df_test = output_df.loc[output_df[datetime_colname] > train_end, ]

    return output_df_train, output_df_test


def main(train_dir, test_dir, output_dir, datetime_colname, holiday_colname):
    """
    This helper function uses the create_basic_features and create_advanced
    features functions to create features for each train and test round.
    """

    output_train_dir = os.path.join(output_dir, 'train')
    output_test_dir = os.path.join(output_dir, 'test')
    if not os.path.isdir(output_train_dir):
        os.mkdir(output_train_dir)
    if not os.path.isdir(output_test_dir):
        os.mkdir(output_test_dir)

    train_base_df = pd.read_csv(os.path.join(train_dir, TRAIN_BASE_FILE),
                                parse_dates=[datetime_colname])

    # These features only need to be created once for all rounds
    train_base_basic_features = \
        create_basic_features(train_base_df,
                              datetime_colname=datetime_colname)

    for i in range(1, NUM_ROUND+1):
        train_file = os.path.join(train_dir, TRAIN_FILE_PREFIX + str(i) + '.csv')
        test_file = os.path.join(test_dir, TEST_FILE_PREFIX + str(i) + '.csv')

        train_delta_df = pd.read_csv(train_file, parse_dates=[datetime_colname])
        test_round_df = pd.read_csv(test_file, parse_dates=[datetime_colname])

        train_delta_basic_features = \
            create_basic_features(train_delta_df,
                                  datetime_colname=datetime_colname)
        test_basic_features = \
            create_basic_features(test_round_df,
                                  datetime_colname=datetime_colname)

        train_round_df = pd.concat([train_base_df, train_delta_df])
        train_advanced_features, test_advanced_features = \
            create_advanced_features(train_round_df, test_round_df,
                                     datetime_colname=datetime_colname,
                                     holiday_colname=holiday_colname)

        train_basic_features = pd.concat([train_base_basic_features,
                                         train_delta_basic_features])

        # Drop some overlapping columns before merge basic and advanced
        # features.
        train_basic_columns = set(train_basic_features.columns)
        train_advanced_columns = set(train_advanced_features.columns)
        train_overlap_columns = list(train_basic_columns.
                                     intersection(train_advanced_columns))
        train_overlap_columns.remove('Zone')
        train_overlap_columns.remove('Datetime')
        train_advanced_features.drop(train_overlap_columns,
                                     inplace=True, axis=1)

        test_basic_columns = set(test_basic_features.columns)
        test_advanced_columns = set(test_advanced_features.columns)
        test_overlap_columns = list(test_basic_columns.
                                    intersection(test_advanced_columns))
        test_overlap_columns.remove('Zone')
        test_overlap_columns.remove('Datetime')
        test_advanced_features.drop(test_overlap_columns, inplace=True, axis=1)

        train_all_features = pd.merge(train_basic_features,
                                      train_advanced_features,
                                      on=['Zone', 'Datetime'])
        test_all_features = pd.merge(test_basic_features,
                                     test_advanced_features,
                                     on=['Zone', 'Datetime'])

        train_all_features.dropna(inplace=True)
        test_all_features.drop(['DewPnt', 'DryBulb', 'DEMAND'],
                               inplace=True, axis=1)

        test_month = test_basic_features['MonthOfYear'].values[0]
        train_all_features = train_all_features.loc[
            train_all_features['MonthOfYear'] == test_month, ].copy()

        train_output_file = os.path.join(output_train_dir,
                                         TRAIN_FILE_PREFIX + str(i) + '.csv')
        test_output_file = os.path.join(output_test_dir,
                                        TEST_FILE_PREFIX + str(i) + '.csv')

        train_all_features.to_csv(train_output_file, index=False)
        test_all_features.to_csv(test_output_file, index=False)

        print('Round {}'.format(i))
        print('Training data size: {}'.format(train_all_features.shape))
        print('Testing data size: {}'.format(test_all_features.shape))
        print('Minimum training timestamp: {}'.
              format(min(train_all_features[datetime_colname])))
        print('Maximum training timestamp: {}'.
              format(max(train_all_features[datetime_colname])))
        print('Minimum testing timestamp: {}'.
              format(min(test_all_features[datetime_colname])))
        print('Maximum testing timestamp: {}'.
              format(max(test_all_features[datetime_colname])))
        print('')


if __name__ == '__main__':
    opts, args = getopt.getopt(sys.argv[1:], '', ['submission='])
    for opt, arg in opts:
        if opt == '--submission':
            submission_folder = arg
            output_data_dir = os.path.join(SUBMISSIONS_DIR, submission_folder, 'data')
            if not os.path.isdir(output_data_dir):
                os.mkdir(output_data_dir)
            OUTPUT_DIR = os.path.join(output_data_dir, 'features')
    if not os.path.isdir(OUTPUT_DIR):
        os.mkdir(OUTPUT_DIR)

    main(train_dir=TRAIN_DATA_DIR,
         test_dir=TEST_DATA_DIR,
         output_dir=OUTPUT_DIR,
         datetime_colname=DATETIME_COLNAME,
         holiday_colname=HOLIDAY_COLNAME)
